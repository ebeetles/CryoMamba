# Test Design: Story 3.2 - Preview Downsampling and Streaming

## Test Strategy Overview

This document outlines the comprehensive testing strategy for the preview downsampling and streaming functionality implemented in Story 3.2.

## Requirements Traceability

### Acceptance Criteria Mapping

| AC | Description | Test Coverage |
|----|-------------|---------------|
| 1 | Real tomogram emits previews every 1–2 seconds during inference | ✅ Integration tests verify streaming frequency |
| 2 | Preview downsampling maintains visual quality | ✅ Unit tests validate downsampling algorithms |
| 3 | WebSocket streaming delivers previews with low latency | ✅ Performance tests measure latency |
| 4 | Preview messages contain proper JSON format with base64 payload | ✅ Unit tests validate message format |
| 5 | Desktop client displays previews in real-time | ✅ E2E tests verify client integration |

## Test Levels

### 1. Unit Tests

**File**: `tests/test_preview_downsampler.py` (to be created)

**Coverage**:
- ✅ All downsampling methods (nearest, average, max, gaussian)
- ✅ Multi-resolution preview generation
- ✅ Visual quality optimization
- ✅ Memory usage estimation
- ✅ Edge cases (empty masks, invalid inputs)

**Test Cases**:
```python
def test_downsample_nearest():
    """Test nearest neighbor downsampling"""
    # Given: 128x128x128 mask
    # When: Downsample using nearest method
    # Then: Result maintains segmentation boundaries

def test_downsample_max():
    """Test max pooling downsampling"""
    # Given: Segmentation mask with boundaries
    # When: Downsample using max method
    # Then: Boundaries are preserved

def test_visual_quality_optimization():
    """Test visual quality improvements"""
    # Given: Noisy segmentation mask
    # When: Apply quality optimization
    # Then: Noise is reduced, boundaries are cleaner
```

### 2. Integration Tests

**File**: `tests/test_preview_streamer.py` (to be created)

**Coverage**:
- ✅ Preview streaming lifecycle (start/stop)
- ✅ Mask updates during streaming
- ✅ Performance monitoring integration
- ✅ Caching behavior
- ✅ Error handling and recovery

**Test Cases**:
```python
async def test_streaming_lifecycle():
    """Test complete streaming lifecycle"""
    # Given: PreviewStreamer instance
    # When: Start streaming, update masks, stop streaming
    # Then: All operations complete successfully

async def test_performance_monitoring():
    """Test performance metrics collection"""
    # Given: Active streaming session
    # When: Multiple preview updates occur
    # Then: Performance metrics are collected accurately
```

### 3. Performance Tests

**File**: `tests/test_preview_performance.py` (to be created)

**Coverage**:
- ✅ Latency measurements
- ✅ Memory usage monitoring
- ✅ Adaptive frequency adjustment
- ✅ Cache hit/miss ratios
- ✅ Throughput under load

**Test Cases**:
```python
def test_adaptive_streaming():
    """Test adaptive frequency adjustment"""
    # Given: High latency scenario
    # When: Adaptive controller processes metrics
    # Then: Frequency is reduced appropriately

def test_cache_performance():
    """Test caching performance benefits"""
    # Given: Repeated preview requests
    # When: Cache is utilized
    # Then: Response time improves significantly
```

### 4. End-to-End Tests

**File**: `tests/test_e2e_preview_streaming.py` (to be created)

**Coverage**:
- ✅ Complete inference pipeline with previews
- ✅ WebSocket communication
- ✅ Desktop client integration
- ✅ Real-time preview display
- ✅ Error scenarios and recovery

**Test Cases**:
```python
async def test_e2e_preview_streaming():
    """Test complete preview streaming workflow"""
    # Given: Running server and desktop client
    # When: Start inference job
    # Then: Previews are streamed and displayed in real-time

async def test_preview_error_recovery():
    """Test error handling in preview streaming"""
    # Given: Streaming session with errors
    # When: Error occurs during preview generation
    # Then: System recovers gracefully
```

## Test Data Management

### Test Masks
- **Small**: 32x32x32 - Fast unit tests
- **Medium**: 128x128x128 - Integration tests
- **Large**: 512x512x512 - Performance tests
- **Edge Cases**: Empty, single voxel, extreme aspect ratios

### Test Scenarios
- **Normal Operation**: Standard segmentation masks
- **Edge Cases**: Empty masks, single voxel, extreme shapes
- **Error Conditions**: Invalid inputs, memory constraints
- **Performance**: High load, concurrent streams

## Mock Strategy

### External Dependencies
- **nnU-Net**: Mock predictor for unit tests
- **WebSocket**: Mock connections for integration tests
- **File System**: Mock file operations for testing
- **Time**: Mock time for performance testing

### Test Doubles
```python
class MockNnUNetPredictor:
    """Mock nnU-Net predictor for testing"""
    def predict_from_files(self, input_files, output_files, **kwargs):
        # Generate predictable test data
        pass

class MockWebSocketConnection:
    """Mock WebSocket connection for testing"""
    async def send_text(self, message):
        # Capture messages for verification
        pass
```

## Test Execution Strategy

### Continuous Integration
- **Unit Tests**: Run on every commit
- **Integration Tests**: Run on pull requests
- **Performance Tests**: Run nightly
- **E2E Tests**: Run on release candidates

### Test Environment
- **Development**: Local testing with mock dependencies
- **CI/CD**: Automated testing with real dependencies
- **Staging**: Full integration testing
- **Production**: Monitoring and alerting

## Quality Gates

### Coverage Requirements
- **Unit Tests**: ≥90% line coverage
- **Integration Tests**: ≥80% path coverage
- **E2E Tests**: All critical user journeys

### Performance Benchmarks
- **Preview Generation**: <100ms for 128x128x128 masks
- **WebSocket Latency**: <50ms end-to-end
- **Memory Usage**: <100MB per streaming session
- **Throughput**: ≥10 previews/second

### Acceptance Criteria
- All ACs must have corresponding test cases
- All tests must pass consistently
- Performance benchmarks must be met
- No critical security vulnerabilities

## Test Maintenance

### Regular Updates
- **Monthly**: Review and update test data
- **Quarterly**: Assess test coverage and effectiveness
- **Annually**: Evaluate testing strategy and tools

### Documentation
- **Test Cases**: Documented with Given-When-Then format
- **Test Data**: Version controlled and documented
- **Results**: Tracked and analyzed for trends

## Risk Assessment

### High Risk Areas
- **WebSocket Reliability**: Network failures, connection drops
- **Memory Management**: Large volume processing, memory leaks
- **Performance Degradation**: Under high load conditions

### Mitigation Strategies
- **Comprehensive Error Handling**: Graceful degradation
- **Resource Monitoring**: Memory and CPU usage tracking
- **Load Testing**: Validate performance under stress
- **Circuit Breakers**: Prevent cascade failures

## Conclusion

This comprehensive testing strategy ensures the preview downsampling and streaming functionality is robust, performant, and reliable. The multi-level approach provides confidence in both individual components and the integrated system.
